#!/bin/bash

set -euo pipefail

BASEDIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && cd .. && pwd)"

# shellcheck source=lib/shared.bash
. "$BASEDIR/lib/shared.bash"

expand_headers_on_error() {
  echo "^^^ +++"
}
trap expand_headers_on_error ERR

if [[ "$(plugin_read_config DEBUG "false")" =~ ^(true|on|1)$ ]]; then
  set -x
fi

if [[ "$(plugin_read_config SAVE "false")" =~ ^(false|off|0)$ ]]; then
  echo "--- Skipping cache save; save not set"
  exit 0
fi
if [ "${BUILDKITE_COMMAND_EXIT_STATUS:-}" -ne 0 ]; then
  echo "--- ðŸš¨ Cache is skipped because step returned ${BUILDKITE_COMMAND_EXIT_STATUS}"
  exit 0
fi

if [[ -n "$(plugin_read_config NAME)" ]]; then
  echo -e "--- :bank: Saving Docker Cache: \033[33m$(plugin_read_config NAME)\033[0m"
else
  echo "--- :bank: Saving Docker Cache"
fi

keys=()
volumes=()
s3_bucket="$(plugin_read_config S3_BUCKET)"
bucket_path="${BUILDKITE_ORGANIZATION_SLUG}/${BUILDKITE_PIPELINE_SLUG}"
cache_dir=cache
override_file="docker-compose.cache-volumes.buildkite-${BUILDKITE_BUILD_NUMBER}-override.yml"

while IFS=$'\n' read -r key ; do
  [[ -n "${key:-}" ]] && keys+=("$(expand_key "$key")")
done <<< "$(plugin_read_list KEYS)"

echo "Read keys: ${keys[@]}"
echo "Using cache key: ${keys[0]}"

s3_key="${bucket_path}/${keys[0]}.tar"
filename="${keys[0]}.tar"

aws s3api head-object --bucket "${s3_bucket}" --key "${s3_key}" || no_head=true
echo

if ${no_head:-false}; then
  cache_save "s3://${s3_bucket}/${s3_key}"

  while IFS=$'\n' read -r volume ; do
    [[ -n "${volume:-}" ]] && volumes+=("$volume")
  done <<< "$(plugin_read_list VOLUMES)"

  test -f "$override_file" && rm "$override_file"
  echo "~~~ :docker: Creating docker-compose override file for volumes"
  build_volume_override_file "${volumes[@]}" | tee "$override_file"

  echo "--- :docker: Exporting docker volumes to cache"

  compose_files=()
  IFS=' ' read -r -a config_files <<< "${DOCKER_COMPOSE_CONFIG_FILES:-}"

  for file in "${config_files[@]}"; do
    [[ -n "${file:-}" ]] && compose_files+=(-f "$file")
  done
  compose_files+=(-f "${override_file}")

  project_name="${DOCKER_COMPOSE_PROJECT_NAME}"

  mkdir -p "${cache_dir}"

  for volume in "${volumes[@]}" ; do
    echo "Exporting data for ${volume}"

    cid=$(plugin_prompt_and_must_run docker-compose \
      "${compose_files[@]}" \
      -p "${project_name}" \
      run \
      --detach \
      docker-cache-buildkite-plugin \
      sleep infinity
    )

    if [[ "$(plugin_read_config VOLUME_DEBUG "false")" =~ ^(true|on|1)$ ]]; then
      plugin_prompt_and_run docker \
        exec \
        "${cid}" \
        ls -la "/volumes/${volume}"
    fi

    plugin_prompt_and_must_run docker \
      exec \
      "${cid}" \
      tar -cf - -C "/volumes/${volume}" . \
      > "cache/${volume}.tar"

    plugin_prompt_and_must_run docker \
      rm -f "${cid}"

  done

  volume_tars=()
  for volume in "${volumes[@]}"; do
    volume_tars+=("${volume}.tar")
  done

  tar -cf "${filename}" -C "${cache_dir}" "${volume_tars[@]}"
  aws s3 cp "${filename}" "s3://${s3_bucket}/${s3_key}"

  rm -f "${filename}"
  rm -rf "${cache_dir}"
else
  cache_save_skip "s3://${s3_bucket}/${s3_key}"
fi
