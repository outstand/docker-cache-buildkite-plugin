#!/bin/bash

set -euo pipefail

BASEDIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && cd .. && pwd)"

# shellcheck source=lib/shared.bash
. "$BASEDIR/lib/shared.bash"

if [[ -z "${BATS_PATH:-}" ]]; then
  # Only source this when not under test. Otherwise it stomps on our stubs.
  # shellcheck source=lib/cache-keys.bash
  . "$BASEDIR/lib/cache-keys.bash"
fi

expand_headers_on_error() {
  echo "^^^ +++"
}
trap expand_headers_on_error ERR

if [[ "$(plugin_read_config DEBUG "false")" =~ ^(true|on|1)$ ]]; then
  set -x
fi

if [[ -n "$(plugin_read_config NAME)" ]]; then
  echo -e "--- :bank: Restoring Docker Cache: \033[33m$(plugin_read_config NAME)\033[0m"
else
  echo "--- :bank: Restoring Docker Cache"
fi

keys=()
volumes=()
s3_bucket="$(plugin_read_config S3_BUCKET)"
bucket_path="${BUILDKITE_ORGANIZATION_SLUG}/${BUILDKITE_PIPELINE_SLUG}"
cache_dir=cache
override_file="docker-compose.cache-volumes.buildkite-${BUILDKITE_BUILD_NUMBER}-override.yml"

while IFS=$'\n' read -r key ; do
  [[ -n "${key:-}" ]] && keys+=("$(expand_key "$key")")
done <<< "$(plugin_read_list KEYS)"


set +e
set -x
resolved_key=$(find_cache "${s3_bucket}" "${bucket_path}" "${keys[@]}")
find_cache_result=$?
if [[ $find_cache_result == 2 ]]; then
  echo "find_cache failed:"
  echo "${resolved_key}"
  exit 2
elif [[ $find_cache_result == 1 ]]; then
  # No resolved key found
  cache_restore_skip "s3://${s3_bucket}/${bucket_path}/${keys[0]}"
  exit 0
fi
set -e

echo "Using cache key: ${resolved_key}"

s3_key="${bucket_path}/${resolved_key}.tar"
filename="${resolved_key}.tar"

cache_hit "s3://${s3_bucket}/${s3_key}"
aws s3 cp "s3://${s3_bucket}/${s3_key}" .

mkdir -p "${cache_dir}"
tar -xf "${filename}" -C "${cache_dir}"

while IFS=$'\n' read -r volume ; do
  [[ -n "${volume:-}" ]] && volumes+=("$volume")
done <<< "$(plugin_read_list VOLUMES)"

test -f "$override_file" && rm "$override_file"
echo "--- :docker: Creating docker-compose override file for volumes"
build_volume_override_file "${volumes[@]}" | tee "$override_file"

echo "--- :docker: Extracting cache into docker volumes"

compose_files=()
IFS=' ' read -r -a config_files <<< "${DOCKER_COMPOSE_CONFIG_FILES:-}"

for file in "${config_files[@]}"; do
  [[ -n "${file:-}" ]] && compose_files+=(-f "$file")
done
compose_files+=(-f "${override_file}")

project_name="${DOCKER_COMPOSE_PROJECT_NAME}"

for volume in "${volumes[@]}" ; do
  if [[ -f "cache/${volume}.tar" ]] ; then
    echo "Extracting data for ${volume}"

    plugin_prompt_and_must_run docker-compose \
      "${compose_files[@]}" \
      -p "${project_name}" \
      run \
      --rm \
      docker-cache-buildkite-plugin \
      tar -xf - -C /volumes/"${volume}" --strip-components=1 \
      < "cache/${volume}.tar"

    if [[ "$(plugin_read_config VOLUME_DEBUG "false")" =~ ^(true|on|1)$ ]]; then
      plugin_prompt_and_run docker-compose \
        "${compose_files[@]}" \
        -p "${project_name}" \
        run \
        --rm \
        docker-cache-buildkite-plugin \
        ls -la "/volumes/${volume}"
    fi
  else
    echo "+++ :warning: Missing volume ${volume} for restore!"
  fi
done

rm -f "${filename}"
rm -rf "${cache_dir}"
